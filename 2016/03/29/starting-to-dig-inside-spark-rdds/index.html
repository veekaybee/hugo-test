<!doctype html><html lang=en-us>
<head>
<meta http-equiv=x-clacks-overhead content="GNU Terry Pratchett">
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<link rel="shortcut icon" href=https://veekaybee.github.io/hugo-test/static/images/favicon.png>
<title>Starting to dig inside Spark RDDs | ★★Vicki Boykis★★</title>
<meta name=title content="Starting to dig inside Spark RDDs">
<meta name=description content="A lot of the Spark documentation has examples using either the Scala spark-shell or the Python pyspark repl. These are really great to test out code fragments, especially for Scala, which needs to be packaged.
So you get a lot of examples like this:
textFile.map(line => line.split(&#34; &#34;).size).reduce((a, b) => if (a > b) a else b)
What&rsquo;s the next step to writing production-grade, reproducible apps?
Let&rsquo;s say we have some log data in HDFS already: Using some fake log data I generated with this excellent script:">
<meta name=keywords content>
<meta property="og:title" content="Starting to dig inside Spark RDDs">
<meta property="og:description" content="A lot of the Spark documentation has examples using either the Scala spark-shell or the Python pyspark repl. These are really great to test out code fragments, especially for Scala, which needs to be packaged.
So you get a lot of examples like this:
textFile.map(line => line.split(&#34; &#34;).size).reduce((a, b) => if (a > b) a else b)
What&rsquo;s the next step to writing production-grade, reproducible apps?
Let&rsquo;s say we have some log data in HDFS already: Using some fake log data I generated with this excellent script:">
<meta property="og:type" content="article">
<meta property="og:url" content="https://veekaybee.github.io/hugo-test/2016/03/29/starting-to-dig-inside-spark-rdds/"><meta property="og:image" content="https://veekaybee.github.io/hugo-test/images/share.png"><meta property="article:section" content="blog">
<meta property="article:published_time" content="2016-03-29T00:00:00+00:00">
<meta property="article:modified_time" content="2016-03-29T00:00:00+00:00"><meta property="og:site_name" content="Hugo ʕ•ᴥ•ʔ Bear">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://veekaybee.github.io/hugo-test/images/share.png">
<meta name=twitter:title content="Starting to dig inside Spark RDDs">
<meta name=twitter:description content="A lot of the Spark documentation has examples using either the Scala spark-shell or the Python pyspark repl. These are really great to test out code fragments, especially for Scala, which needs to be packaged.
So you get a lot of examples like this:
textFile.map(line => line.split(&#34; &#34;).size).reduce((a, b) => if (a > b) a else b)
What&rsquo;s the next step to writing production-grade, reproducible apps?
Let&rsquo;s say we have some log data in HDFS already: Using some fake log data I generated with this excellent script:">
<meta name=twitter:site content="@vboykis">
<meta itemprop=name content="Starting to dig inside Spark RDDs">
<meta itemprop=description content="A lot of the Spark documentation has examples using either the Scala spark-shell or the Python pyspark repl. These are really great to test out code fragments, especially for Scala, which needs to be packaged.
So you get a lot of examples like this:
textFile.map(line => line.split(&#34; &#34;).size).reduce((a, b) => if (a > b) a else b)
What&rsquo;s the next step to writing production-grade, reproducible apps?
Let&rsquo;s say we have some log data in HDFS already: Using some fake log data I generated with this excellent script:"><meta itemprop=datePublished content="2016-03-29T00:00:00+00:00">
<meta itemprop=dateModified content="2016-03-29T00:00:00+00:00">
<meta itemprop=wordCount content="664"><meta itemprop=image content="https://veekaybee.github.io/hugo-test/images/share.png">
<meta itemprop=keywords content>
<meta name=referrer content="no-referrer-when-downgrade">
<style>body{font-family:Verdana,sans-serif;margin:auto;padding:20px;max-width:720px;text-align:left;background-color:#fff;word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:#444}h1,h2,h3,h4,h5,h6,strong,b{color:#222}a{color:#3273dc}.title{text-decoration:none;border:0}.title span{font-weight:400}nav a{margin-right:10px}textarea{width:100%;font-size:16px}input{font-size:16px}content{line-height:1.6}table{width:100%}img{max-width:100%}code{padding:2px 5px;background-color:#f2f2f2}pre code{color:#222;display:block;padding:20px;white-space:pre-wrap;font-size:14px}div.highlight pre{background-color:initial;color:initial}div.highlight code{background-color:unset;color:unset}blockquote{border-left:1px solid #999;color:#222;padding-left:20px;font-style:italic}footer{padding:25px;text-align:center}.helptext{color:#777;font-size:small}.errorlist{color:#eba613;font-size:small}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:#8b6fcb}@media(prefers-color-scheme:dark){body{background-color:#333;color:#ddd}h1,h2,h3,h4,h5,h6,strong,b{color:#eee}a{color:#8cc2dd}code{background-color:#777}pre code{color:#ddd}blockquote{color:#ccc}textarea,input{background-color:#252525;color:#ddd}.helptext{color:#aaa}}</style>
</head>
<body>
<header><a href=/hugo-test/ class=title>
<h2>★★Vicki Boykis★★</h2>
</a>
<nav><a href=/hugo-test/>Tech Blog</a>
<a href=/hugo-test/essays>Essays</a>
<a href=/hugo-test/index.xml>RSS</a>
<a href=https://www.twitter.com/vboykis>Twitter</a>
<a href=https://www.github.com/veekaybee>GitHub</a>
<a href=/hugo-test/about>About</a></nav>
</header>
<main>
<h1>Starting to dig inside Spark RDDs</h1>
<p>
<i>
<time datetime=2016-03-29 pubdate>
Mar 3 2016
</time>
</i>
</p>
<content>
<p>A lot of the Spark documentation has examples using either the Scala <code>spark-shell</code> or the Python <code>pyspark</code> repl. These are really great to test out code fragments, especially for Scala, which needs to be packaged.</p>
<p>So you get a lot of examples <a href=http://spark.apache.org/docs/latest/quick-start.html>like this</a>:</p>
<p><code>textFile.map(line => line.split(" ").size).reduce((a, b) => if (a > b) a else b)</code></p>
<p>What&rsquo;s the next step to writing production-grade, reproducible apps?</p>
<p>Let&rsquo;s say we have some log data in HDFS already: Using some fake log data I generated with <a href=https://gist.github.com/gwenshap/11390102>this excellent script</a>:</p>
<p>Setting up the file for Spark analysis:</p>
<pre><code>hdfs dfs -mkdir /logdata
hdfs dfs -put access_log_20160329-153657.log /logdata
hdfs dfs -cat /logdata/access_log_20160329-153657.log | tail -n 1

16.180.70.237 - - [10/Oct/2013:23:25:41 ] &quot;GET /handle-bars HTTP/1.0&quot; 200 2780 &quot;http://www.casualcyclist.com&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1944.0 Safari/537.36&quot;
</code></pre>
<p>And let&rsquo;s say we want to count the number of times each IP appears.</p>
<p>In Python, it&rsquo;s relatively easy to write apps.</p>
<p>The steps are:</p>
<ol>
<li>Write your .py file.</li>
<li>Run on the command line</li>
<li>Spend the rest of your time relaxing on the beach.</li>
</ol>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>
<span style=color:#f92672>import</span> sys
<span style=color:#f92672>from</span> pyspark <span style=color:#f92672>import</span> SparkContext
<span style=color:#f92672>from</span> pyspark <span style=color:#f92672>import</span> SparkConf

<span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
    <span style=color:#66d9ef>if</span> len(sys<span style=color:#f92672>.</span>argv) <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>2</span>:
        print <span style=color:#f92672>&gt;&gt;</span> sys<span style=color:#f92672>.</span>stderr, <span style=color:#e6db74>&#34;Usage: IPs &lt;file&gt;&#34;</span>
        exit(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)

sc <span style=color:#f92672>=</span> SparkContext()

logs <span style=color:#f92672>=</span> sys<span style=color:#f92672>.</span>argv[<span style=color:#ae81ff>1</span>]

logs <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>textFile(<span style=color:#e6db74>&#34;/logdata/access_log_20160329-153657.log&#34;</span>)
iplines <span style=color:#f92672>=</span> logs<span style=color:#f92672>.</span>map(<span style=color:#66d9ef>lambda</span> lines: (lines<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#39; &#39;</span>)[<span style=color:#ae81ff>0</span>],<span style=color:#ae81ff>1</span>))<span style=color:#f92672>.</span>reduceByKey(<span style=color:#66d9ef>lambda</span> v1,v2 :v1<span style=color:#f92672>+</span>v2)

<span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> iplines<span style=color:#f92672>.</span>take(<span style=color:#ae81ff>10</span>):
	print <span style=color:#e6db74>&#34;IP:&#34;</span> , i[<span style=color:#ae81ff>0</span>], <span style=color:#e6db74>&#34;Count:&#34;</span>, i[<span style=color:#ae81ff>1</span>]


sc<span style=color:#f92672>.</span>stop()
</code></pre></div><p>and then:</p>
<p>```spark-submit &ndash;master yarn-client countIp.py /logdata/*``</p>
<p>In Scala, it&rsquo;s a little more complicated:</p>
<ol>
<li>Write your Scala code</li>
<li><a href=https://sparktutorials.github.io/2015/04/02/setting-up-a-spark-project-with-maven.html>Build the correct POM</a> (not going to cover this in this post)</li>
<li>Compile with Maven</li>
<li>Run compiled job in the command line</li>
<li>Weep</li>
</ol>
<p>OR</p>
<ol start=2>
<li>Build with SBT.</li>
<li>Run compiled job in the command line</li>
<li>Weep</li>
</ol>
<p>The root file:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=color:#75715e>//whatever you want to run this file as
</span><span style=color:#75715e></span><span style=color:#66d9ef>package</span> name

<span style=color:#66d9ef>import</span> org.apache.spark.SparkContext
<span style=color:#66d9ef>import</span> org.apache.spark.SparkContext._
<span style=color:#66d9ef>import</span> org.apache.spark.SparkConf

<span style=color:#66d9ef>object</span> <span style=color:#a6e22e>CountIPs</span> <span style=color:#f92672>{</span>
   <span style=color:#66d9ef>def</span> main<span style=color:#f92672>(</span>args<span style=color:#66d9ef>:</span> <span style=color:#66d9ef>Array</span><span style=color:#f92672>[</span><span style=color:#66d9ef>String</span><span style=color:#f92672>])</span> <span style=color:#f92672>{</span>
     <span style=color:#66d9ef>if</span> <span style=color:#f92672>(</span>args<span style=color:#f92672>.</span>length <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>1</span><span style=color:#f92672>)</span> <span style=color:#f92672>{</span>
       <span style=color:#a6e22e>System</span><span style=color:#f92672>.</span>err<span style=color:#f92672>.</span>println<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Usage: CountIPs &lt;file&gt;&#34;</span><span style=color:#f92672>)</span>
       <span style=color:#a6e22e>System</span><span style=color:#f92672>.</span>exit<span style=color:#f92672>(</span><span style=color:#ae81ff>1</span><span style=color:#f92672>)</span>
     <span style=color:#f92672>}</span>
    <span style=color:#75715e>//Parse log files from HDFS to get IP, count of IPs
</span><span style=color:#75715e></span>	<span style=color:#66d9ef>var</span> logs <span style=color:#66d9ef>=</span> sc<span style=color:#f92672>.</span>textFile<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;/logdata/access_log_20160329-153657.log&#34;</span><span style=color:#f92672>)</span>
	<span style=color:#66d9ef>var</span> iplines <span style=color:#66d9ef>=</span> logs<span style=color:#f92672>.</span>map<span style=color:#f92672>(</span>lines <span style=color:#66d9ef>=&gt;</span> <span style=color:#f92672>(</span>lines<span style=color:#f92672>.</span>split<span style=color:#f92672>(</span><span style=color:#e6db74>&#39; &#39;</span><span style=color:#f92672>)(</span><span style=color:#ae81ff>0</span><span style=color:#f92672>),</span><span style=color:#ae81ff>1</span><span style=color:#f92672>)).</span>reduceByKey<span style=color:#f92672>((</span>a<span style=color:#f92672>,</span> b<span style=color:#f92672>)</span> <span style=color:#66d9ef>=&gt;</span> a <span style=color:#f92672>+</span> b<span style=color:#f92672>)</span>

	<span style=color:#75715e>//print out to console
</span><span style=color:#75715e></span>	<span style=color:#66d9ef>for</span> <span style=color:#f92672>(</span>pair <span style=color:#66d9ef>&lt;-</span> iplines<span style=color:#f92672>.</span>take<span style=color:#f92672>(</span><span style=color:#ae81ff>10</span><span style=color:#f92672>))</span> <span style=color:#f92672>{</span>
   printf<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;IP: %s, Count: %s\n&#34;</span><span style=color:#f92672>,</span>pair<span style=color:#f92672>.</span>_1<span style=color:#f92672>,</span>pair<span style=color:#f92672>.</span>_2<span style=color:#f92672>)</span>
<span style=color:#f92672>}</span>



	sc<span style=color:#f92672>.</span>stop
	<span style=color:#f92672>}</span>
  <span style=color:#f92672>}</span>	
</code></pre></div><p>And the cli options:</p>
<p><code>spark-submit \
--class name.CountIps \
--master yarn-client \ target/countIps-1.0.jar /logdata/*</code></p>
<p>While working through the Scala example, I learned something that was interesting to me as someone who is new to Scala, coming from a Python background, related to var/val assignment.</p>
<p>Scala has several basic language variable <a href=http://www.scala-lang.org/files/archive/spec/2.11/04-basic-declarations-and-definitions.html>declarations</a>:</p>
<pre><code>Dcl      ::=  ‘val’ ValDcl
          |  ‘var’ VarDcl
          |  ‘def’ FunDcl
          |  ‘type’ {nl} TypeDcl
</code></pre>
<p>several of which are particularly of interest to Spark users: <code>val</code> and <code>var</code> .</p>
<p><code>val</code> allows for the creation of immutable (i.e. read-only) references and <code>var</code> references can be overwritten.</p>
<p>Spark RDDs by nature are <a href=https://spark.apache.org/docs/0.8.1/api/core/org/apache/spark/rdd/RDD.html>immutable</a>, and documentation and best practices have them assigned to <code>val</code> types.</p>
<p>So it surprised me to see some documentation in a recent Spark class I took assigning RDDs to <code>vars</code> instead of <code>vals</code> within an app that was not reusing RDDs. And inherently, because of the way Spark internals are structured, it doesn&rsquo;t make sense, or at least I haven&rsquo;t yet come across a use case, where you need to change the assignment of one RDD when you can just create a new one through a transformation.</p>
<p>I looked into why you might ostensibly use var over val within the context of Spark&rsquo;s immutable RDD paragigm, and didn&rsquo;t find anything specific, until I saw this note in Advanced Analytics with Spark:</p>
<p><img src=https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/master/images/varval.png alt=image></p>
<p>So, basically it&rsquo;s ok when you&rsquo;re writing throw-away commands in the REPL where it doesn&rsquo;t make much of a difference.</p>
<p>Digging through <a href=https://community.hortonworks.com/questions/18708/are-spark-rdd-really-mutable.html>the Spark documentation</a>, as well as <a href="https://mail-archives.apache.org/mod_mbox/spark-user/201602.mbox/%3CCALte62wXf5jSQUpzsr=zYayw0D-L5+tPVONE7fqsdnC=Ne59cQ@mail.gmail.com%3E">the Spark mailing list</a> (an excellent resource for learning Spark) has led me to believe that using var in Spark is an anti-pattern in Scala apps.</p>
<blockquote>
<p>Although individual RDDs are immutable, it is possible to implement mutable state by having multiple RDDs to represent multiple versions of a dataset.</p>
</blockquote>
</content>
<p>
</p>
</main>
<footer>Made with <a href=https://github.com/janraasch/hugo-bearblog/>Hugo ʕ•ᴥ•ʔ Bear</a>
</footer>
</body>
</html>