<!doctype html><html lang=en-us>
<head>
<meta http-equiv=x-clacks-overhead content="GNU Terry Pratchett">
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<link rel="shortcut icon" href=https://veekaybee.github.io/hugo-test/images/favicon.png>
<title>Should Spark have an API for R? | ★★Vicki Boykis★★</title>
<meta name=title content="Should Spark have an API for R?">
<meta name=description content="Table of Contents
 The SparkR API: DataFrames are confusing Functionality, adoption, and commit velocity Inability to plot data Visibility into debugging Incomplete Documentation Conclusion  Goya, Fire at Night
As a consultant, I&rsquo;m often asked to make tooling choices in big data projects: Should we use Python or R? What&rsquo;s the best NoSQL datastore right now? How do we set up a data lake?
While I do keep up with data news, I don&rsquo;t know everything that&rsquo;s going on in the big data ecosystem, which now sprawls to over 2000 companies and products, and neither does anyone else working in the space.">
<meta name=keywords content>
<meta property="og:title" content="Should Spark have an API for R?">
<meta property="og:description" content="Table of Contents
 The SparkR API: DataFrames are confusing Functionality, adoption, and commit velocity Inability to plot data Visibility into debugging Incomplete Documentation Conclusion  Goya, Fire at Night
As a consultant, I&rsquo;m often asked to make tooling choices in big data projects: Should we use Python or R? What&rsquo;s the best NoSQL datastore right now? How do we set up a data lake?
While I do keep up with data news, I don&rsquo;t know everything that&rsquo;s going on in the big data ecosystem, which now sprawls to over 2000 companies and products, and neither does anyone else working in the space.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://veekaybee.github.io/hugo-test/2017/07/20/should-spark-have-an-api-for-r/"><meta property="og:image" content="https://veekaybee.github.io/hugo-test/images/share.png"><meta property="article:section" content="blog">
<meta property="article:published_time" content="2017-07-20T00:00:00+00:00">
<meta property="article:modified_time" content="2017-07-20T00:00:00+00:00"><meta property="og:site_name" content="Hugo ʕ•ᴥ•ʔ Bear">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://veekaybee.github.io/hugo-test/images/share.png">
<meta name=twitter:title content="Should Spark have an API for R?">
<meta name=twitter:description content="Table of Contents
 The SparkR API: DataFrames are confusing Functionality, adoption, and commit velocity Inability to plot data Visibility into debugging Incomplete Documentation Conclusion  Goya, Fire at Night
As a consultant, I&rsquo;m often asked to make tooling choices in big data projects: Should we use Python or R? What&rsquo;s the best NoSQL datastore right now? How do we set up a data lake?
While I do keep up with data news, I don&rsquo;t know everything that&rsquo;s going on in the big data ecosystem, which now sprawls to over 2000 companies and products, and neither does anyone else working in the space.">
<meta itemprop=name content="Should Spark have an API for R?">
<meta itemprop=description content="Table of Contents
 The SparkR API: DataFrames are confusing Functionality, adoption, and commit velocity Inability to plot data Visibility into debugging Incomplete Documentation Conclusion  Goya, Fire at Night
As a consultant, I&rsquo;m often asked to make tooling choices in big data projects: Should we use Python or R? What&rsquo;s the best NoSQL datastore right now? How do we set up a data lake?
While I do keep up with data news, I don&rsquo;t know everything that&rsquo;s going on in the big data ecosystem, which now sprawls to over 2000 companies and products, and neither does anyone else working in the space."><meta itemprop=datePublished content="2017-07-20T00:00:00+00:00">
<meta itemprop=dateModified content="2017-07-20T00:00:00+00:00">
<meta itemprop=wordCount content="3899"><meta itemprop=image content="https://veekaybee.github.io/hugo-test/images/share.png">
<meta itemprop=keywords content>
<meta name=referrer content="no-referrer-when-downgrade">
<style>body{font-family:Verdana,sans-serif;margin:auto;padding:20px;max-width:720px;text-align:left;background-color:#fff;word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:#444}h1,h2,h3,h4,h5,h6,strong,b{color:#222}a{color:#3273dc}.title{text-decoration:none;border:0}.title span{font-weight:400}nav a{margin-right:10px}textarea{width:100%;font-size:16px}input{font-size:16px}content{line-height:1.6}table{width:100%}img{max-width:100%}code{padding:2px 5px;background-color:#f2f2f2}pre code{color:#222;display:block;padding:20px;white-space:pre-wrap;font-size:14px}div.highlight pre{background-color:initial;color:initial}div.highlight code{background-color:unset;color:unset}blockquote{border-left:1px solid #999;color:#222;padding-left:20px;font-style:italic}footer{padding:25px;text-align:center}.helptext{color:#777;font-size:small}.errorlist{color:#eba613;font-size:small}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:#8b6fcb}@media(prefers-color-scheme:dark){body{background-color:#333;color:#ddd}h1,h2,h3,h4,h5,h6,strong,b{color:#eee}a{color:#8cc2dd}code{background-color:#777}pre code{color:#ddd}blockquote{color:#ccc}textarea,input{background-color:#252525;color:#ddd}.helptext{color:#aaa}}</style>
</head>
<body>
<header><a href=/hugo-test/ class=title>
<h2>★★Vicki Boykis★★</h2>
</a>
<nav><a href=/hugo-test/>Tech Blog</a>
<a href=/hugo-test/essays>Essays</a>
<a href=/hugo-test/index.xml>RSS</a>
<a href=https://www.twitter.com/vboykis>Twitter</a>
<a href=https://www.github.com/veekaybee>GitHub</a>
<a href=/hugo-test/about>About</a></nav>
</header>
<main>
<h1>Should Spark have an API for R?</h1>
<p>
<i>
<time datetime=2017-07-20 pubdate>
Jul 7 2017
</time>
</i>
</p>
<content>
<p><strong>Table of Contents</strong></p>
<ul>
<li><a href=#the-sparkr-api-dataframes-are-confusing>The SparkR API: DataFrames are confusing</a></li>
<li><a href=#functionality-adoption-and-commit-velocity>Functionality, adoption, and commit velocity</a></li>
<li><a href=#inability-to-plot-data>Inability to plot data</a></li>
<li><a href=#visibility-into-debugging>Visibility into debugging</a></li>
<li><a href=#incomplete-documentation>Incomplete Documentation</a></li>
<li><a href=#conclusion>Conclusion</a></li>
</ul>
<p><img src=https://uploads4.wikiart.org/images/francisco-goya/fire-at-night-1794.jpg alt=spark>
Goya, Fire at Night</p>
<p>As a consultant, I&rsquo;m often asked to make tooling choices in big data projects: Should we use Python or R? What&rsquo;s the best NoSQL datastore right now? How do we set up a data lake?</p>
<p>While I do <a href=http://veekaybee.github.io/getting-tech-industry-news/>keep up with data news</a>, I don&rsquo;t know everything that&rsquo;s going on in the big data ecosystem, which now sprawls to over <a href=http://veekaybee.github.io/senior-dev-day-talk/#/4>2000 companies and products</a>, and neither does anyone else working in the space.</p>
<p>In an effort to make all of these parts accessible to different audiences, developers create APIs that the majority of data scientists already use, namely by making their code accessible to R and Python interfaces.</p>
<p>A number of products are based off the success of tooling for these two languages. Microsoft, for example, offers R and Jupyter notebooks in <a href=http://veekaybee.github.io/using-azure-ml/>Azure Machine Learning</a>. Amazon offers <a href=https://aws.amazon.com/blogs/big-data/running-jupyter-notebook-and-jupyterhub-on-amazon-emr/>Jupyter notebooks</a> on EMR. And, in my favorite example about how everyone is trying to get on the data science bandwagon, SAS now lets you run <a href=https://support.sas.com/rnd/app/studio/Rinterface2.html>R inside SAS</a>.</p>
<p>One project that has been particularly good at staying ahead of the curve is <a href=https://spark.apache.org/>Spark</a>, which began in Scala and quickly added a fully-functioning Python API when data scientists without JVM experience started moving to the platform. It&rsquo;s currently under the aegis of <a href=https://en.wikipedia.org/wiki/Databricks>Databricks</a>, a company started by the original Spark project developers. It provides, of course, <a href=https://databricks.com/product/databricks>its own notebook product</a>.</p>
<p>The motivation behind SparkR, is great. If you know R, you don&rsquo;t have to switch to Python or Scala to create your models, while also benefitting from the processing power of Spark. But, SparkR, as it stands now, is not (yet) a fully-featured mirror of R functionality, and I&rsquo;ve come across some key features of the platform that make me believe that SparkR is not a good fit for the Spark programming paradigm.</p>
<h1 id=the-sparkr-api-dataframes-are-confusing>The SparkR API: DataFrames are confusing</h1>
<p>The first problem is data organization. Data frames have become a common element across all languages that deal with data, mimicking the functionality of a table in a SQL database. <a href=https://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe>Python</a> <a href=http://www.r-tutor.com/r-introduction/data-frame>R</a>, <a href=http://support.sas.com/documentation/cdl/en/lrcon/62955/HTML/default/viewer.htm#a001005709.htm>SAS</a> all have them, and <a href=https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html>Spark</a> has followed suit.</p>
<p>In general, tables are good. Tables are the way humans read data easiest, and the way we manipulate it. In relational environments, tables are pretty straightforward. But the issues start when imperative programming languages try to mimic declarative data structures, particularly when these data structures are distributed across many machines and linked together by complicated sets of instructions.</p>
<p>In Spark, there big difference between <code>SparkDataFrames</code>, and R&rsquo;s <code>data.frames</code>.</p>
<p>Here&rsquo;s a high-level overview of the differences:</p>
<p>To the naked eye, they look similar and you can run similar operations on them. There is nothing in the way they are named to tell you what kind of object they are. The difference between them became so confusing that <a href=https://github.com/apache/spark/commit/a55fbe2a16aa0866ff8aca25bf9f772e6eb516a1>SparkR renamed DataFrames</a> to <code>SparkDataFrames</code>, in part because they were competing with the name of a third package, S4Vector.</p>
<p>But, to the person coming from base R functionality, there is a real clash of concepts here. <code>data.frames</code> are similar in the real world to looking up a single word in a dictionary. But with <code>SparkDataFrames</code>, the definition of your word is spread out across all of the volumes of the Encyclopedia Britannica, and you need to search through each volume, get the word you want, and then piece it back together into the definition. Also, the words are not in alphabetical order.</p>
<p>It helps to examine each structure&rsquo;s internals. To start with, let&rsquo;s look at R <code>data.frames</code>.</p>
<p>An <a href=https://cran.r-project.org/doc/manuals/r-release/R-lang.html#Basic-types>R dataframe</a> is an in-memory object created <a href=https://cran.r-project.org/doc/manuals/r-release/R-lang.html#Data-frame-objects>out of two-dimensional lists</a> of vectors of equal lengths, where each column contains values of a single variable and rows contain a single set of observations.</p>
<p>Working in RStudio, we can import one of the default datasets:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R><span style=color:#a6e22e>datasets</span>() <span style=color:#75715e># display available datasets attached to R</span>
<span style=color:#a6e22e>data</span>(income) <span style=color:#75715e># use US family income from US census 2008</span>
<span style=color:#a6e22e>help</span>(income) <span style=color:#75715e># find out more about the package</span>
income <span style=color:#f92672>&lt;-</span>income <span style=color:#75715e># put the data into a data.frame</span>

<span style=color:#f92672>&gt;</span> <span style=color:#a6e22e>class</span>(income) <span style=color:#75715e># check type</span>
[1] <span style=color:#e6db74>&#34;data.frame
</span><span style=color:#e6db74>
</span><span style=color:#e6db74>&gt; class(income$value) #check individual column (vector) type
</span><span style=color:#e6db74>[1] &#34;</span>integer<span style=color:#e6db74>&#34;</span>

<span style=color:#a6e22e>str</span>(income) <span style=color:#75715e># check values</span>
<span style=color:#e6db74>&#39;data.frame&#39;</span><span style=color:#f92672>:</span>	<span style=color:#ae81ff>44</span> obs. of  <span style=color:#ae81ff>4</span> variables<span style=color:#f92672>:</span>
 <span style=color:#f92672>$</span> value<span style=color:#f92672>:</span> int  <span style=color:#ae81ff>0</span> <span style=color:#ae81ff>2500</span> <span style=color:#ae81ff>5000</span> <span style=color:#ae81ff>7500</span> <span style=color:#ae81ff>10000</span> <span style=color:#ae81ff>12500</span> <span style=color:#ae81ff>15000</span> <span style=color:#ae81ff>17500</span> <span style=color:#ae81ff>20000</span> <span style=color:#ae81ff>22500</span> <span style=color:#66d9ef>...</span>
 <span style=color:#f92672>$</span> count<span style=color:#f92672>:</span> int  <span style=color:#ae81ff>2588</span> <span style=color:#ae81ff>971</span> <span style=color:#ae81ff>1677</span> <span style=color:#ae81ff>3141</span> <span style=color:#ae81ff>3684</span> <span style=color:#ae81ff>3163</span> <span style=color:#ae81ff>3600</span> <span style=color:#ae81ff>3116</span> <span style=color:#ae81ff>3967</span> <span style=color:#ae81ff>3117</span> <span style=color:#66d9ef>...</span>
 <span style=color:#f92672>$</span> mean <span style=color:#f92672>:</span> int  <span style=color:#ae81ff>298</span> <span style=color:#ae81ff>3792</span> <span style=color:#ae81ff>6261</span> <span style=color:#ae81ff>8705</span> <span style=color:#ae81ff>11223</span> <span style=color:#ae81ff>13687</span> <span style=color:#ae81ff>16074</span> <span style=color:#ae81ff>18662</span> <span style=color:#ae81ff>21064</span> <span style=color:#ae81ff>23698</span> <span style=color:#66d9ef>...</span>
 <span style=color:#f92672>$</span> prop <span style=color:#f92672>:</span> num  <span style=color:#ae81ff>0.02209</span> <span style=color:#ae81ff>0.00829</span> <span style=color:#ae81ff>0.01431</span> <span style=color:#ae81ff>0.0268</span> <span style=color:#ae81ff>0.03144</span> <span style=color:#66d9ef>...</span>
</code></pre></div><p>When you run operations on an R data.frame, you&rsquo;re processing everything on the machine that is running the R process, in this case, my MacBook:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>mbp-vboykis:~ vboykis$ ps
  PID TTY           TIME CMD
<span style=color:#ae81ff>28239</span> ttys000    0:00.18 ~/R
</code></pre></div><p>A <code>SparkDataFrame</code>, is a very different animal. From <a href=https://databricks.com/>Databricks</a>:</p>
<p><img src=https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/master/images/sparkr.png alt=sparkr></p>
<p>First, <a href=https://www.gitbook.com/book/jaceklaskowski/mastering-apache-spark/details>Spark is distributed</a>, which means processes, instead of happening locally, are broken out across multiple nodes, called workers, across multiple processes, called executors.</p>
<p>The process that kickstarts this is the driver. The driver is a program that kicks off the Spark session and creates the execution plan to submit to the master, which sends it off to the workers. In simpler terms, the driver is the process that kicks off the <code>main()</code> method.</p>
<p>The driver creates a SparkContext, which is the entry point into a Spark program, and splits up code execution on the executors, which are located on separate logical nodes, often also separate physical servers.</p>
<p>You write some SparkR code on the edge node of a Hadoop cluster. That code is translated from SparkR, to a JVM process on the driver (since Spark is Scala under the hood.) This code then gets pushed to JVM processes listening on each edge node. In the process, the data is serialized and split up across machines.</p>
<p>Only at this point, once Spark has been initialized and there is a network connection, a <code>SparkDataFrame</code> is created.
Really, a <a href=https://github.com/apache/spark/blob/v2.1.1/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala>SparkDataFrame is a view</a> of another Spark object, called a <code>Dataset</code>, which is <a href=https://spark.apache.org/docs/2.1.0/api/java/org/apache/spark/sql/Dataset.html>a collection of the serialized JVM objects objects</a>.</p>
<p>The <code>Dataset</code>, the original set of instructions created by translating the R code to Spark, is then formatted in such a way by SparkSQL as to <a href=https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html>mimic a table</a>. There is an <a href="https://www.youtube.com/watch?v=EBNZZuM7bCA&feature=youtu.be">great, detailed talk about this process</a> by one of the SparkR committers.</p>
<p>Additionally, for a <code>SparkDataFrame</code> to compute correctly, Spark has to implement logic across servers. To make this work, the <code>SparkDataFrame</code> is not really the data, but a set of instructions for how to access the data and process it across nodes, the data <a href=https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-rdd-lineage.html>lineage</a>.</p>
<p>Here&rsquo;s a really good visual of all the setup that needs to happen when you do invoke and make changes to a <code>SparkDataFrame</code>, from Databricks:</p>
<p><img src=https://databricks.com/wp-content/uploads/2015/04/Screen-Shot-2015-04-12-at-8.41.26-AM-1024x235.png alt=dspark></p>
<p>There are two major implications for this level of complexity is that you access <code>SparkDataFrames</code> differently than <code>data.frames</code>.</p>
<p><a href=https://github.com/veekaybee/sparkr-examples/blob/master/sparktest.R>Code here. </a></p>
<ol>
<li>Although Spark is a row-based paradigm, you can&rsquo;t access specific rows in an ordered manner in a <code>SparkDataFrame</code>.</li>
</ol>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R><span style=color:#a6e22e>head</span>(income)
  value count  mean        prop
<span style=color:#ae81ff>1</span>     <span style=color:#ae81ff>0</span>  <span style=color:#ae81ff>2588</span>   <span style=color:#ae81ff>298</span> <span style=color:#ae81ff>0.022085115</span>
<span style=color:#ae81ff>2</span>  <span style=color:#ae81ff>2500</span>   <span style=color:#ae81ff>971</span>  <span style=color:#ae81ff>3792</span> <span style=color:#ae81ff>0.008286185</span>
<span style=color:#ae81ff>3</span>  <span style=color:#ae81ff>5000</span>  <span style=color:#ae81ff>1677</span>  <span style=color:#ae81ff>6261</span> <span style=color:#ae81ff>0.014310950</span>
<span style=color:#ae81ff>4</span>  <span style=color:#ae81ff>7500</span>  <span style=color:#ae81ff>3141</span>  <span style=color:#ae81ff>8705</span> <span style=color:#ae81ff>0.026804229</span>
<span style=color:#ae81ff>5</span> <span style=color:#ae81ff>10000</span>  <span style=color:#ae81ff>3684</span> <span style=color:#ae81ff>11223</span> <span style=color:#ae81ff>0.031438007</span>
<span style=color:#ae81ff>6</span> <span style=color:#ae81ff>12500</span>  <span style=color:#ae81ff>3163</span> <span style=color:#ae81ff>13687</span> <span style=color:#ae81ff>0.026991970</span>

<span style=color:#f92672>&gt;</span> <span style=color:#a6e22e>class</span>(income)
[1] <span style=color:#e6db74>&#34;data.frame&#34;</span>

<span style=color:#f92672>&gt;</span> income<span style=color:#a6e22e>[row.names</span>(income)<span style=color:#f92672>==</span><span style=color:#ae81ff>2</span>,]
  value count mean        prop
<span style=color:#ae81ff>2</span>  <span style=color:#ae81ff>2500</span>   <span style=color:#ae81ff>971</span> <span style=color:#ae81ff>3792</span> <span style=color:#ae81ff>0.008286185</span>

</code></pre></div><p>And now the SparkR equivalent:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R>sdf <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>as.DataFrame</span>(income) <span style=color:#75715e>#convert to SparkDataFrame</span>
sdf <span style=color:#75715e>#check out type</span>
SparkDataFrame[value<span style=color:#f92672>:</span>int, count<span style=color:#f92672>:</span>int, mean<span style=color:#f92672>:</span>int, prop<span style=color:#f92672>:</span>double]
<span style=color:#a6e22e>head</span>(sdf) <span style=color:#75715e>#same function as on local data</span>
income<span style=color:#a6e22e>[row.names</span>(income)<span style=color:#f92672>==</span><span style=color:#ae81ff>2</span>,]
Error in sdf<span style=color:#a6e22e>[row.names</span>(income) <span style=color:#f92672>==</span> <span style=color:#ae81ff>2</span>, ] <span style=color:#f92672>:</span>
  Expressions other than filtering predicates are 
not supported in the first parameter of extract operator [ or <span style=color:#a6e22e>subset</span>() method.

</code></pre></div><p>That error comes <a href=https://github.com/apache/spark/blob/74ac1fb081e9532d77278a4edca9f3f129fd62eb/R/pkg/R/DataFrame.R#L1876>from here</a>, which means you can only do columnar operations to an initial <code>DataFrame. </code></p>
<p>So, instead, you iterate over an entire SparkDataFrame as a whole, applying all the changes to what in R would be the equivalent of a vector.</p>
<p>This leads to really annoying workarounds to common R problems, like, for example, filling nulls. Let&rsquo;s say you have a column of nulls, and in order to avoid outliers, you want to impute the average of the column and backfill it.</p>
<p>In an R <code>data.frame</code>, you could do:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R>m <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>matrix</span>(<span style=color:#a6e22e>sample</span>(<span style=color:#a6e22e>c</span>(<span style=color:#66d9ef>NA</span>, <span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#ae81ff>10</span>), <span style=color:#ae81ff>100</span>, replace <span style=color:#f92672>=</span> <span style=color:#66d9ef>TRUE</span>), <span style=color:#ae81ff>10</span>)
d <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>as.data.frame</span>(m)

V1 V2 V3 V4 V5 V6 V7 V8 V9 V10
<span style=color:#ae81ff>1</span>   <span style=color:#ae81ff>1</span>  <span style=color:#ae81ff>4</span> <span style=color:#ae81ff>10</span>  <span style=color:#ae81ff>3</span>  <span style=color:#ae81ff>1</span>  <span style=color:#ae81ff>4</span>  <span style=color:#ae81ff>9</span>  <span style=color:#ae81ff>6</span>  <span style=color:#ae81ff>3</span>   <span style=color:#ae81ff>1</span>
<span style=color:#ae81ff>2</span>   <span style=color:#ae81ff>3</span>  <span style=color:#ae81ff>4</span> <span style=color:#ae81ff>10</span>  <span style=color:#ae81ff>5</span>  <span style=color:#ae81ff>2</span>  <span style=color:#ae81ff>3</span>  <span style=color:#ae81ff>7</span>  <span style=color:#ae81ff>1</span>  <span style=color:#ae81ff>9</span>  <span style=color:#66d9ef>NA</span>
<span style=color:#ae81ff>3</span>  <span style=color:#ae81ff>10</span> <span style=color:#66d9ef>NA</span>  <span style=color:#ae81ff>5</span> <span style=color:#66d9ef>NA</span> <span style=color:#ae81ff>10</span>  <span style=color:#ae81ff>9</span>  <span style=color:#ae81ff>3</span> <span style=color:#66d9ef>NA</span> <span style=color:#66d9ef>NA</span>   <span style=color:#ae81ff>5</span>
<span style=color:#ae81ff>4</span>   <span style=color:#ae81ff>1</span>  <span style=color:#ae81ff>9</span>  <span style=color:#ae81ff>4</span>  <span style=color:#ae81ff>3</span>  <span style=color:#ae81ff>3</span>  <span style=color:#ae81ff>2</span>  <span style=color:#ae81ff>8</span>  <span style=color:#ae81ff>7</span>  <span style=color:#ae81ff>7</span>   <span style=color:#ae81ff>8</span>
<span style=color:#ae81ff>5</span>   <span style=color:#ae81ff>6</span>  <span style=color:#ae81ff>3</span>  <span style=color:#ae81ff>2</span>  <span style=color:#ae81ff>6</span>  <span style=color:#ae81ff>5</span> <span style=color:#ae81ff>10</span>  <span style=color:#ae81ff>5</span> <span style=color:#ae81ff>10</span> <span style=color:#ae81ff>10</span>   <span style=color:#ae81ff>9</span>
<span style=color:#ae81ff>6</span>   <span style=color:#ae81ff>5</span>  <span style=color:#ae81ff>9</span>  <span style=color:#ae81ff>7</span> <span style=color:#ae81ff>10</span>  <span style=color:#ae81ff>5</span>  <span style=color:#ae81ff>6</span>  <span style=color:#ae81ff>8</span>  <span style=color:#ae81ff>3</span>  <span style=color:#ae81ff>4</span>  <span style=color:#ae81ff>10</span>
<span style=color:#ae81ff>7</span>   <span style=color:#ae81ff>9</span> <span style=color:#ae81ff>10</span>  <span style=color:#ae81ff>3</span>  <span style=color:#ae81ff>6</span>  <span style=color:#ae81ff>6</span>  <span style=color:#ae81ff>4</span>  <span style=color:#ae81ff>6</span>  <span style=color:#ae81ff>7</span>  <span style=color:#ae81ff>7</span>   <span style=color:#ae81ff>8</span>
<span style=color:#ae81ff>8</span>   <span style=color:#ae81ff>5</span>  <span style=color:#ae81ff>9</span>  <span style=color:#ae81ff>8</span>  <span style=color:#ae81ff>4</span>  <span style=color:#ae81ff>8</span>  <span style=color:#ae81ff>2</span>  <span style=color:#ae81ff>2</span>  <span style=color:#ae81ff>9</span>  <span style=color:#ae81ff>9</span>  <span style=color:#66d9ef>NA</span>
<span style=color:#ae81ff>9</span>   <span style=color:#ae81ff>1</span>  <span style=color:#ae81ff>5</span>  <span style=color:#ae81ff>1</span>  <span style=color:#ae81ff>8</span>  <span style=color:#ae81ff>7</span>  <span style=color:#ae81ff>3</span>  <span style=color:#ae81ff>8</span>  <span style=color:#ae81ff>1</span> <span style=color:#66d9ef>NA</span>   <span style=color:#ae81ff>4</span>
<span style=color:#ae81ff>10</span> <span style=color:#ae81ff>10</span>  <span style=color:#ae81ff>2</span>  <span style=color:#ae81ff>9</span> <span style=color:#ae81ff>10</span>  <span style=color:#ae81ff>1</span>  <span style=color:#ae81ff>3</span>  <span style=color:#ae81ff>8</span>  <span style=color:#ae81ff>8</span>  <span style=color:#ae81ff>5</span>   <span style=color:#ae81ff>6</span>

<span style=color:#a6e22e>for</span>(i in <span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#a6e22e>ncol</span>(d)){
  d<span style=color:#a6e22e>[is.na</span>(d[,i]), i] <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>mean</span>(d[,i], na.rm <span style=color:#f92672>=</span> <span style=color:#66d9ef>TRUE</span>)
}

d

V1        V2 V3        V4 V5 V6 V7        V8    V9    V10
<span style=color:#ae81ff>1</span>   <span style=color:#ae81ff>1</span>  <span style=color:#ae81ff>4.000000</span> <span style=color:#ae81ff>10</span>  <span style=color:#ae81ff>3.000000</span>  <span style=color:#ae81ff>1</span>  <span style=color:#ae81ff>4</span>  <span style=color:#ae81ff>9</span>  <span style=color:#ae81ff>6.000000</span>  <span style=color:#ae81ff>3.00</span>  <span style=color:#ae81ff>1.000</span>
<span style=color:#ae81ff>2</span>   <span style=color:#ae81ff>3</span>  <span style=color:#ae81ff>4.000000</span> <span style=color:#ae81ff>10</span>  <span style=color:#ae81ff>5.000000</span>  <span style=color:#ae81ff>2</span>  <span style=color:#ae81ff>3</span>  <span style=color:#ae81ff>7</span>  <span style=color:#ae81ff>1.000000</span>  <span style=color:#ae81ff>9.00</span>  <span style=color:#ae81ff>6.375</span>
<span style=color:#ae81ff>3</span>  <span style=color:#ae81ff>10</span>  <span style=color:#ae81ff>6.111111</span>  <span style=color:#ae81ff>5</span>  <span style=color:#ae81ff>6.111111</span> <span style=color:#ae81ff>10</span>  <span style=color:#ae81ff>9</span>  <span style=color:#ae81ff>3</span>  <span style=color:#ae81ff>5.777778</span>  <span style=color:#ae81ff>6.75</span>  <span style=color:#ae81ff>5.000</span>
<span style=color:#ae81ff>4</span>   <span style=color:#ae81ff>1</span>  <span style=color:#ae81ff>9.000000</span>  <span style=color:#ae81ff>4</span>  <span style=color:#ae81ff>3.000000</span>  <span style=color:#ae81ff>3</span>  <span style=color:#ae81ff>2</span>  <span style=color:#ae81ff>8</span>  <span style=color:#ae81ff>7.000000</span>  <span style=color:#ae81ff>7.00</span>  <span style=color:#ae81ff>8.000</span>
<span style=color:#ae81ff>5</span>   <span style=color:#ae81ff>6</span>  <span style=color:#ae81ff>3.000000</span>  <span style=color:#ae81ff>2</span>  <span style=color:#ae81ff>6.000000</span>  <span style=color:#ae81ff>5</span> <span style=color:#ae81ff>10</span>  <span style=color:#ae81ff>5</span> <span style=color:#ae81ff>10.000000</span> <span style=color:#ae81ff>10.00</span>  <span style=color:#ae81ff>9.000</span>
<span style=color:#ae81ff>6</span>   <span style=color:#ae81ff>5</span>  <span style=color:#ae81ff>9.000000</span>  <span style=color:#ae81ff>7</span> <span style=color:#ae81ff>10.000000</span>  <span style=color:#ae81ff>5</span>  <span style=color:#ae81ff>6</span>  <span style=color:#ae81ff>8</span>  <span style=color:#ae81ff>3.000000</span>  <span style=color:#ae81ff>4.00</span> <span style=color:#ae81ff>10.000</span>
<span style=color:#ae81ff>7</span>   <span style=color:#ae81ff>9</span> <span style=color:#ae81ff>10.000000</span>  <span style=color:#ae81ff>3</span>  <span style=color:#ae81ff>6.000000</span>  <span style=color:#ae81ff>6</span>  <span style=color:#ae81ff>4</span>  <span style=color:#ae81ff>6</span>  <span style=color:#ae81ff>7.000000</span>  <span style=color:#ae81ff>7.00</span>  <span style=color:#ae81ff>8.000</span>
<span style=color:#ae81ff>8</span>   <span style=color:#ae81ff>5</span>  <span style=color:#ae81ff>9.000000</span>  <span style=color:#ae81ff>8</span>  <span style=color:#ae81ff>4.000000</span>  <span style=color:#ae81ff>8</span>  <span style=color:#ae81ff>2</span>  <span style=color:#ae81ff>2</span>  <span style=color:#ae81ff>9.000000</span>  <span style=color:#ae81ff>9.00</span>  <span style=color:#ae81ff>6.375</span>
<span style=color:#ae81ff>9</span>   <span style=color:#ae81ff>1</span>  <span style=color:#ae81ff>5.000000</span>  <span style=color:#ae81ff>1</span>  <span style=color:#ae81ff>8.000000</span>  <span style=color:#ae81ff>7</span>  <span style=color:#ae81ff>3</span>  <span style=color:#ae81ff>8</span>  <span style=color:#ae81ff>1.000000</span>  <span style=color:#ae81ff>6.75</span>  <span style=color:#ae81ff>4.000</span>
<span style=color:#ae81ff>10</span> <span style=color:#ae81ff>10</span>  <span style=color:#ae81ff>2.000000</span>  <span style=color:#ae81ff>9</span> <span style=color:#ae81ff>10.000000</span>  <span style=color:#ae81ff>1</span>  <span style=color:#ae81ff>3</span>  <span style=color:#ae81ff>8</span>  <span style=color:#ae81ff>8.000000</span>  <span style=color:#ae81ff>5.00</span>  <span style=color:#ae81ff>6.000</span>
</code></pre></div><p>In SparkR, you have to work around the fact that you can&rsquo;t fill columns the same way (due to partitioning, you can&rsquo;t aggregate.)</p>
<p>Here&rsquo;s the error message:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R>Error in sdfD<span style=color:#a6e22e>[is.na</span>(sdfD[, i]), i] <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>mean</span>(d[, i], na.rm <span style=color:#f92672>=</span> <span style=color:#66d9ef>TRUE</span>) <span style=color:#f92672>:</span>
  object of type <span style=color:#e6db74>&#39;S4&#39;</span> is not subsettable
In addition<span style=color:#f92672>:</span> Warning message<span style=color:#f92672>:</span>
In <span style=color:#a6e22e>is.na</span>(sdfD[, i]) <span style=color:#f92672>:</span> 
<span style=color:#a6e22e>is.na</span>() applied to non<span style=color:#f92672>-</span>(list or vector) of type <span style=color:#e6db74>&#39;S4&#39;</span>
</code></pre></div><p>So you have to force a variable from SparkR and fill with the <code>fillna</code>, as a list, for every column:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R><span style=color:#a6e22e>mean</span>(sdfD<span style=color:#f92672>$</span>v10)
your_average<span style=color:#f92672>&lt;-</span><span style=color:#a6e22e>as.list</span>(<span style=color:#a6e22e>head</span>(<span style=color:#a6e22e>select</span>(sdfD,<span style=color:#a6e22e>mean</span>(sdfD<span style=color:#f92672>$</span>v10))))
sdfFinal<span style=color:#f92672>&lt;-</span><span style=color:#a6e22e>fillna</span>(sdfD,<span style=color:#a6e22e>list</span>(<span style=color:#e6db74>&#34;v10&#34;</span> <span style=color:#f92672>=</span> your_average[[1]]))
sdfFinal
<span style=color:#a6e22e>head</span>(sdfFinal)
</code></pre></div><p>It&rsquo;s arguably less code (at least for a single entity), but it&rsquo;s much harder to understand, and it takes a lot of digging through the API to find these types of equivalents.</p>
<p>And, second, unlike R, which will return the same result set every time you want to display a <code>data.frame</code>, the <a href=https://stackoverflow.com/questions/41364540/saving-ordered-dataframe-in-spark>distributed nature and partitioning process in Spark</a>, means each pass at a <code>SparkDataFrame</code> <a href=https://stackoverflow.com/questions/29792320/spark-sort-by-key-and-then-group-by-to-get-ordered-iterable>returns a slightly different subset of data when you&rsquo;re doing computations</a>, depending on how quickly processes complete and how the data execution plan decides to act.</p>
<p>What happens when data is returned from Spark is really well-illustrated by <a href=https://nedbatchelder.com/blog/201204/two_problems.html>this joke</a>:</p>
<blockquote>
<p>Some people, when confronted with a problem, think &ldquo;I know, I&rsquo;ll use multithreading&rdquo;. Nothhw tpe yawrve o oblems.</p>
</blockquote>
<p>To add to this confusion, you can switch between <code>SparkDataFrames</code> and <code>data.frames</code> and not even be aware until Spark throws an exception.</p>
<p>And you can write similar functions for both. So what is, in theory, a great feature, language portability, becomes a hassle, because you can have:</p>
<p><code>model &lt;- glm(F ~ x1+x2+x3, df, family = "gaussian")</code></p>
<p><code>model &lt;- glm(F ~ x1+x2+x3,data=df,family=gaussian())</code></p>
<p>Which one is R, and which one is Spark? If you forget syntax, it&rsquo;s easy to get confused.</p>
<p>Which is why, a common practice I&rsquo;ve seen is labeling our objects with sdf for <code>SparkDataFrame</code> and rdf for r <code>data.frames</code>.</p>
<p><code>sdfmodel &lt;- glm(F ~ x1+x2+x3, df, family = "gaussian")</code></p>
<p><code>rdfmodel &lt;- glm(F ~ x1+x2+x3,data=df,family=gaussian())</code></p>
<p>This gets confusing and a bit cumbersome if you end up creating a lot of objects, or having to pass objects from one environment to the other.</p>
<p>Which brings me to the second issue:</p>
<h1 id=functionality-adoption-and-commit-velocity>Functionality, adoption, and commit velocity</h1>
<p>SparkR is young and, ostensibly, growing. That means that features are constantly being added. But SparkR, while growing, seems to be a lower-priority language in the Spark ecosystem.</p>
<p>You can see in Matei Zaharia&rsquo;s <a href=https://www.slideshare.net/SparkSummit/trends-for-big-data-and-apache-spark-in-2017-by-matei-zaharia>slides</a> at <a href=https://www.slideshare.net/databricks/expanding-apache-spark-use-cases-in-22-and-beyond-with-matei-zaharia-and-demos-by-michael-armbrust-and-tim-hunter>SparkSummit</a> over the past couple years, that Databricks, the company now overseeing Spark development, is more concerned about catching up to the market for deep learning, streaming, and fine-tuning SparkSQL performance (which impacts Scala, Python, and R), than focusing on on the SparkR API.</p>
<p>This makes sense: only 20% of Spark language usage is in R,leading to a chicken-and-egg problem. Until that number grows, there&rsquo;s no pressure to significantly up resources devoted to it. Until there are more resources devoted to it, the number of users won&rsquo;t increase.</p>
<p>Aside from messaging from leadership, is there a way to tell whether SparkR development is increasing and whether it makes sense to use SparkR for a project? I pulled the SparkR codebase mirrored from Apache on <a href=https://github.com/apache/spark>GitHub</a> to find out.</p>
<p>A cURl call tells us that SparkR makes up less than 4% of the overall Spark codebase (in lines of code):</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e># pull all languages used in the Spark repo</span>

curl -u veekaybee -G <span style=color:#e6db74>&#34;https://api.github.com/repos/apache/spark/languages&#34;</span>
Enter host password <span style=color:#66d9ef>for</span> user <span style=color:#e6db74>&#39;veekaybee&#39;</span>:
<span style=color:#f92672>{</span>
  <span style=color:#e6db74>&#34;Scala&#34;</span>: 22832829,
  <span style=color:#e6db74>&#34;Java&#34;</span>: 2948574,
  <span style=color:#e6db74>&#34;Python&#34;</span>: 2210161,
  <span style=color:#e6db74>&#34;R&#34;</span>: 1047322,
  <span style=color:#e6db74>&#34;Shell&#34;</span>: 155167,
  <span style=color:#e6db74>&#34;JavaScript&#34;</span>: 140987,
  <span style=color:#e6db74>&#34;Thrift&#34;</span>: 33605,
  <span style=color:#e6db74>&#34;ANTLR&#34;</span>: 32969,
  <span style=color:#e6db74>&#34;Batchfile&#34;</span>: 24294,
  <span style=color:#e6db74>&#34;CSS&#34;</span>: 23957,
  <span style=color:#e6db74>&#34;Roff&#34;</span>: 14420,
  <span style=color:#e6db74>&#34;HTML&#34;</span>: 9800,
  <span style=color:#e6db74>&#34;Makefile&#34;</span>: 7774,
  <span style=color:#e6db74>&#34;PLpgSQL&#34;</span>: 6763,
  <span style=color:#e6db74>&#34;SQLPL&#34;</span>: 6233,
  <span style=color:#e6db74>&#34;PowerShell&#34;</span>: 3751,
  <span style=color:#e6db74>&#34;C&#34;</span>: <span style=color:#ae81ff>1493</span>
<span style=color:#f92672>}</span>
</code></pre></div><p><img src=https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/master/images/codebase.png alt=sparkr></p>
<p>Granted, this itself isn&rsquo;t an indicator of anything. There are a couple of theories here:</p>
<ol>
<li>
<p>Most of the interop code to get R to run with Scala is written in Scala. Looking at the Python codebase, we see a similar pattern: In spite of how prevalent PySpark usage is, it makes up only 7% of the Spark source code.</p>
</li>
<li>
<p>SparkR development started very recently, and will take a significant amount of time to catch up to the volume of code written in Scala.</p>
</li>
<li>
<p>R is more terse than Scala or even Python and requires less LOC to get the job done.</p>
</li>
</ol>
<p>To dig a little further, I cloned the Spark repository and parsed out the Git logs in Python. Script available <a href=https://github.com/veekaybee/sparkr-examples/blob/master/pull_commits.py>here</a>.</p>
<p>This worked pretty well, since the commit messages are really well-organized and tagged (at least, since 2013 ;), as a result of <a href=https://spark.apache.org/contributing.html>Spark&rsquo;s extensively documented contribution process</a> and standardized pull request language:</p>
<blockquote>
<p>The PR title should be of the form [SPARK-xxxx][COMPONENT] Title, where SPARK-xxxx is the relevant JIRA number, COMPONENT is one of the PR categories shown at spark-prs.appspot.com and Title may be the JIRA’s title or a more specific title describing the PR itself.</p>
</blockquote>
<p>Checking out the pull request categories <a href=https://spark-prs.appspot.com/>on the reference site</a>,reveals a ton of work being done in SQL, and not as much in R, but there is no spatial or time element, meaning it&rsquo;s again hard to evaluate.</p>
<p><img src=https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/master/images/prs.png alt=sparkr></p>
<p>Here are the total commits to the Spark project over time:</p>
<p><img src=https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/master/images/total_commits.png alt=sparkr></p>
<p>And here is PySpark vs SparkR:</p>
<p><img src=https://raw.githubusercontent.com/veekaybee/veekaybee.github.io/master/images/issuesmergedbytype.png alt=sparkr></p>
<p>I left out 2017 since it&rsquo;s not complete yet. Surprisingly, visually there seems to be more activity in SparkR than PySpark over the same period. This in itself is also not indicative of anything specific. Does it mean that more resources are being devoted to it, or that it&rsquo;s so far behind that more commits are needed to catch it up?</p>
<p>Once again, hard to tell. But, when working with R, it&rsquo;s easy to intuit that there are a number of features in R that are missing in SparkR. One of the most important one of which is <code>apply</code>.</p>
<p>In R, apply acts as a nice substitute for having to loop over a data.frame by <a href=https://nsaunders.wordpress.com/2010/08/20/a-brief-introduction-to-apply-in-r/>working over the margins,</a> or row/columnar boundaries, to act on individual &ldquo;cells.&rdquo;</p>
<p>SparkR can&rsquo;t work with objects at the cellular level because of its distributed nature, and the passing back and forth between JVM processes ,which means it&rsquo;s <a href=https://github.com/rstudio/sparklyr/issues/81>just now starting</a> to implement apply.</p>
<p>There are a couple of SparkR native functions, <a href=https://spark.apache.org/docs/2.0.0/sparkr.html#applying-user-defined-function>gapply and dapply</a>, but neither of those do the same exact work as apply does, and there are all sorts of workarounds that don&rsquo;t always get there.</p>
<p>And finally, model objects don&rsquo;t have nearly the same accessibility as their analogs in R.</p>
<p>For example, one of the standard outputs for K-means clustering in R is WSS and available components, objects of the model easily accessible:</p>
<p>(check out the <a href=https://stat.ethz.ch/R-manual/R-devel/library/stats/html/kmeans.html>R API</a> and <a href=https://spark.apache.org/docs/2.0.0/api/R/spark.kmeans.html>SparkR API</a> for kmeans)</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R><span style=color:#a6e22e>set.seed</span>(<span style=color:#ae81ff>20</span>)
irisCluster <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>kmeans</span>(iris[, <span style=color:#ae81ff>3</span><span style=color:#f92672>:</span><span style=color:#ae81ff>4</span>], <span style=color:#ae81ff>3</span>, nstart <span style=color:#f92672>=</span> <span style=color:#ae81ff>20</span>)
irisCluster
K<span style=color:#f92672>-</span>means clustering with <span style=color:#ae81ff>3</span> clusters of sizes <span style=color:#ae81ff>46</span>, <span style=color:#ae81ff>54</span>, <span style=color:#ae81ff>50</span>

Cluster means<span style=color:#f92672>:</span>
  Petal.Length Petal.Width
<span style=color:#ae81ff>1</span>     <span style=color:#ae81ff>5.626087</span>    <span style=color:#ae81ff>2.047826</span>
<span style=color:#ae81ff>2</span>     <span style=color:#ae81ff>4.292593</span>    <span style=color:#ae81ff>1.359259</span>
<span style=color:#ae81ff>3</span>     <span style=color:#ae81ff>1.462000</span>    <span style=color:#ae81ff>0.246000</span>

Clustering vector<span style=color:#f92672>:</span>
  [1] <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span>
 [35] <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>3</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span>
 [69] <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span>
[103] <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span>
[137] <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>2</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span> <span style=color:#ae81ff>1</span>

Within cluster sum of squares by cluster<span style=color:#f92672>:</span>
[1] <span style=color:#ae81ff>15.16348</span> <span style=color:#ae81ff>14.22741</span>  <span style=color:#ae81ff>2.02200</span>
 (between_SS <span style=color:#f92672>/</span> total_SS <span style=color:#f92672>=</span>  <span style=color:#ae81ff>94.3</span> %)

Available components<span style=color:#f92672>:</span>

[1] <span style=color:#e6db74>&#34;cluster&#34;</span>      <span style=color:#e6db74>&#34;centers&#34;</span>      <span style=color:#e6db74>&#34;totss&#34;</span>        <span style=color:#e6db74>&#34;withinss&#34;</span>    
[5] <span style=color:#e6db74>&#34;tot.withinss&#34;</span> <span style=color:#e6db74>&#34;betweenss&#34;</span>    <span style=color:#e6db74>&#34;size&#34;</span>         <span style=color:#e6db74>&#34;iter&#34;</span>        
[9] <span style=color:#e6db74>&#34;ifault&#34;</span>
</code></pre></div><p>SparkR does not give WSS as an output, and does not make model objects available in the API, which means a workaround where code is pushed down locally is required to find out the optimal cluster size:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R>model <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>spark.kmeans</span>(df, <span style=color:#f92672>~</span> Petal_Length <span style=color:#f92672>+</span> Petal_Width, k <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>, initMode <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;random&#34;</span>)
<span style=color:#a6e22e>summary</span>(model)
[1] <span style=color:#ae81ff>3</span>

<span style=color:#f92672>$</span>coefficients
  Petal_Length Petal_Width
<span style=color:#ae81ff>1</span> <span style=color:#ae81ff>4.925253</span>     <span style=color:#ae81ff>1.681818</span>   
<span style=color:#ae81ff>2</span> <span style=color:#ae81ff>1.492157</span>     <span style=color:#ae81ff>0.2627451</span>  

<span style=color:#f92672>$</span>size
<span style=color:#f92672>$</span>size[[1]]
[1] <span style=color:#ae81ff>99</span>

<span style=color:#f92672>$</span>size[[2]]
[1] <span style=color:#ae81ff>51</span>

<span style=color:#f92672>$</span>size[[3]]
[1] <span style=color:#ae81ff>0</span>

<span style=color:#f92672>$</span>cluster
SparkDataFrame[prediction<span style=color:#f92672>:</span>int]

<span style=color:#f92672>$</span>is.loaded
[1] <span style=color:#66d9ef>FALSE</span>

<span style=color:#f92672>$</span>clusterSize
[1] <span style=color:#ae81ff>2</span>

<span style=color:#75715e>## Workaround for WSS:</span>
wss <span style=color:#f92672>&lt;-</span> (<span style=color:#a6e22e>nrow</span>(rdfsample)<span style=color:#ae81ff>-1</span>)<span style=color:#f92672>*</span><span style=color:#a6e22e>sum</span>(<span style=color:#a6e22e>apply</span>(rdfsample,<span style=color:#ae81ff>2</span>,var))

<span style=color:#a6e22e>for </span>(i in <span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#ae81ff>9999</span>) wss[i] <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>sum</span>(
  <span style=color:#a6e22e>kmeans</span>(rdfsample, centers<span style=color:#f92672>=</span>i)<span style=color:#f92672>$</span>withinss) 

<span style=color:#a6e22e>plot</span>(<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#ae81ff>9999</span>,
    wss,
    type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;b&#34;</span>,
    xlab<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Number of Clusters&#34;</span>,
    ylab<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Within groups sum of squares&#34;</span>, xlim<span style=color:#f92672>=</span><span style=color:#a6e22e>c</span>(<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>10</span>))
</code></pre></div><h1 id=inability-to-plot-data>Inability to plot data</h1>
<p>Plotting data is the key to exploring and understanding for data science, and R is very good at it. If I have a project that&rsquo;s small enough to fit on my laptop and has no external dependencies, I&rsquo;ll usually choose R over Python for plotting, because, although <a href=http://pythonplot.com/>Python has so many possibilities</a>, they are not as well-developed as R.</p>
<p>SparkR <a href=https://spark-summit.org/east-2016/events/ggplot2sparkr-rebooting-ggplot2-for-scalable-big-data-visualization/>has</a> a port of ggplot2. Unfortunately, <a href=https://github.com/SKKU-SKT/ggplot2.SparkR>this project</a> hasn&rsquo;t been updated in a year, and is <a href=https://github.com/SKKU-SKT/ggplot2.SparkR/blob/master/DESCRIPTION#L23>no longer compatible</a> with the latest version of Spark.</p>
<p>This means, if you want to visualize something, you have to push it down into a <code>data.frame</code>. In general, this makes sense: Spark means processing hundreds of thousands, potentially millions of rows of data which means you have to summarize the data you&rsquo;re working with before you do so. But, even when you summarize, you can&rsquo;t visualize a subset in SparkR, interrupting your workflow.</p>
<p>Or, you could potentially push to a <a href=https://zeppelin.apache.org/docs/0.6.2/interpreter/spark.html>Zeppelin notebook</a>, which means additional configuration.</p>
<p>Neither of those options are optimal for exploratory data analysis, a crucial task when working with data in any system.</p>
<p>Granted, plotting is a bit raw for all Spark APIs, but this is not the case in PySpark, which has <a href=https://plot.ly/python/apache-spark/>Plotly</a>.</p>
<h1 id=visibility-into-debugging>Visibility into debugging</h1>
<p><a href="https://www.youtube.com/watch?v=EBNZZuM7bCA&feature=youtu.be">A lot of things happen</a> when local R code is converted to a SparkDataFrame and as it makes its way back.</p>
<ol>
<li>R opens port and waits for connections</li>
<li>SparkR establishes connections</li>
<li>Each SparkR call sends serialized data over local connection and wait for response</li>
<li>Methods done by the JVM process</li>
<li>R-> JVM serialized binary data</li>
<li>Types converted to lists</li>
<li>Method + arguments are serialized, sent to backend</li>
<li>The method is resolved.</li>
</ol>
<p>This means that if something goes wrong in your code, it could be in any one of the following places:</p>
<ol>
<li>R native code</li>
<li>Serialization of data to SparkR</li>
<li>Conversion to JVM</li>
<li>Movement from driver to executors</li>
<li>Actual Spark job</li>
<li>results</li>
</ol>
<p>This is a completely different paradigm for R users, particularly ones not familiar with distributed computing, to understand. There is a huge learning scale, potentially larger than for engineers with no statistical background coming from other languages. The serialization/deserialization process means that there are many layers you need to understand and dig into when your code errors out.</p>
<p>The problem could be in the SparkR syntax; i.e. a function is unsupported. The issue could be in the serialization to JVM code. The issue could be in the network. Or, the problem could be in the still-developing SparkR codebase.</p>
<p>In order to understand errors, it&rsquo;s important to become familiar with <a href=https://stackoverflow.com/questions/12688068/how-to-read-and-understand-the-java-stack-trace>Java stacktraces</a>, which are completely different from the error codes that R generates.</p>
<h1 id=incomplete-documentation>Incomplete Documentation</h1>
<p>All of these are major issues that lead me to believe that potentially having R on the Spark platform is not generally a good fit for trying to square the circle between distributed computing and R.</p>
<p>But, all of this wouldn&rsquo;t be so bad SparkR had better documentation.</p>
<p>As Holden Karau notes in <a href=http://shop.oreilly.com/product/0636920046967.do>&ldquo;High Performance Spark&rdquo;</a>, Spark documentation can be uneven. As with all young projects, most of the documentation is either in the source code, or on the project&rsquo;s page. To add to this, the <a href=https://spark.apache.org/docs/latest/api/R/index.html>SparkR API</a> documentation is good, but not as detailed as PySpark or Scala.</p>
<p>The <a href=https://spark.apache.org/docs/latest/sparkr.html>documentation</a> available on Apache is somewhat comprehensive, but does not give nearly enough examples, and is missing a few big caveats that make it hard for a beginner to navigate.</p>
<p>This is particularly daunting for beginners coming from statistical computing who are trying to understand both how Spark works, how SparkR works, and nuances of the API.</p>
<p>The biggest issue I found missing was the lack of clear documentation that the SparkSQL context <a href=https://spark.apache.org/docs/latest/sparkr.html#upgrading-from-sparkr-16x-to-20>had been deprecated</a> in Spark 2.0:</p>
<blockquote>
<p>Spark’s SQLContext and HiveContext have been deprecated to be replaced by SparkSession. Instead of sparkR.init(), call sparkR.session() in its place to instantiate the SparkSession. Once that is done, that currently active SparkSession will be used for SparkDataFrame operations.</p>
</blockquote>
<blockquote>
<p>The sqlContext parameter is no longer required for these functions: createDataFrame, as.DataFrame, read.json, jsonFile, read.parquet, parquetFile, read.text, sql, tables, tableNames, cacheTable, uncacheTable, clearCache, dropTempTable, read.df, loadDF, createExternalTable.</p>
</blockquote>
<p>and there are a lot of dead-end answers to that effect that still reference Spark 1.6 on StackOverflow.</p>
<p>Another issue that I spent a lot of time chasing down was <a href=https://spark.apache.org/docs/latest/sparkr.html#r-function-name-conflicts>name conflicts</a>, which, like other caveats, should be easier to read and clearer up-front.</p>
<h1 id=conclusion>Conclusion</h1>
<p>SparkR holds a lot of promise as a gateway into distributed computing for data scientists who are used to the R/RStudio workflow. But, the problem is that the way R works and the way Spark works are orthogonal to one another, and it&rsquo;s not clear that it makes sense to try to &ldquo;parallelize&rdquo; them.</p>
<p>Hopefully the commit velocity means this is a project that&rsquo;s of priority to Spark maintainers, and that the largest issues, including those of inaccessible model objects and additions to the modeling API, as well as clearer documentation, are resolved.</p>
<p>Thank you to <a href=http://www.ednit.net/>Mark Roddy</a> and <a href=https://about.me/jowanza>Jowanza Joseph</a> for reading versions of this.</p>
</content>
<p>
</p>
</main>
<footer>Made with <a href=https://github.com/janraasch/hugo-bearblog/>Hugo ʕ•ᴥ•ʔ Bear</a>
</footer>
</body>
</html>